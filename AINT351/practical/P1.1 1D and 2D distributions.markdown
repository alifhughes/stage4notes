---
title: AINT351 - P1.1 1D and 2D distributions
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LO,LE]{Alistair Hughes - 10421408}
---








## 1. Generate a uniform probability distribution

![uniform-probability-distribution-code](../img/uniform-probability-distribution-code.png)\

![uniform-probability-distribution-graph](../img/uniform-probability-distribution-graph.png)\

The task was to create a uniform probability distribution graph which demonstrates the theory that all data points that are equal in number of occurrences, have an equal probability to be chosen. This is partly because we fill the matrix with data, no other calculations, therefore all numbers is likely to appear the same amount of times.

Changing the number of bins increases the size of the intervals on the x-axis, meaning that the data point values have a larger group to fall in. This pushes the occurrence of each bin much higher the less bins there are. If I were to change the bin from 1000 to 100 then the data point occurrence range goes from the maximum of 25 to 140. This, effectively, decreasing the accuracy. It also means that the peaks and dips of the ranges have a greater difference between them.

The number of samples that have been used has been consistent throughout all graph generations, standing at 10,000 values. This is large enough for there it to have a good range of values even at lower levels of bins but not high enough so that the generation of the graph takes too long nor that the graph is over saturated with values that are unnecessary to prove the point of the graph.

## 2. The central limit theorem

![central-limit-theorem-code](../img/central-limit-theorem-code.png)\

![central-limit-theorem-graph](../img/central-limit-theorem-graph.png)\

The task was to create a central limit theorem graph which demonstrates the theorem that the mean of a large number of iterations of data that are independent, will approximately be normally distributed. That is, no matter data distribution, the mean will gravitate toward the middle point of the data and will have a greater probability than that of the data closer to the limits of data boundaries.

Increasing the size of data used increases the height of the centre of the distribution, where as decreasing not only shortens the height of the centre but widens the standard deviation from the centre of the distribution. Keeping the same number of samples but decreasing the number of bins from 1000 to 10 dramatically changes the appearance of the graph. Although the rise of the curve from outer limits to the centre limit is already steep, decreasing the number of bins only emphasises that feature. As always, it pushes the data occurrences limit up greatly due to there being more chance of falling in a bin.

I think that 10,000 adequately show the purpose of the graph to a good degree of accuracy.

## 3. Generate a normal probability distribution

![normal-probability-distribution-code](../img/normal-probability-distribution-code.png)\

![normal-probability-distribution-graph](../img/normal-probability-distribution-graph.png)\

The task was to create a normal probability distribution graph which demonstrates that the data mean is greatest at its median. Many categories of data fall into this distribution such as average height, average weight, etc. This graph was generated like this because of the use of randn function, which generates randomly normally distributed numbers.

If I were to double the amount of samples used from 10,000 to 20,000, the occurrence of the data at the median greatly increases, almost doubles in fact, but the graph does not widen. The same is for the decreasing of the number of bins, it does not widen the graph, but only heighten the peak at the median. This is because I've restricted the range in which the random numbers were generated in to 5 and -5, so it only further proves that adding more data to such distribution only strengthens the concept that it the mean of it all is falls close to the median.

I believe that between 10,000 and 20,000 is more than enough data points to show the distribution's main characteristics.

## 4. Estimate a normal distribution parameters

![normal-probability-distribution-parameters-code](../img/normal-distribution-parameters-code.png)\

![normal-probability-distribution-parameters-graph](../img/normal-distribution-parameters-graph.png)\

The task was to plot a scaled version of estimated mean and variance of the samples against a scaled normal distribution probability graph. As explained previously, the normal probability distribution graph shows that when data is normally distributed, its mean is greatest at the median, giving the Gaussian or "bell" shaped curve when plotted. The estimated Gaussian distribution was created using MatLab's "normfit()" function which returns the estimated mean and standard variance of the data passed in. Then the data had to be scaled to fit in the data occurrence scale provided. I achieved the right scaling parameters purely by trail and error, I was unsure if there was a formula that gives the right scaling values for both graphs for them to match up. The estimated Gaussian line gives a line of best fit on the raw data samples when they are scaled the same.

Decreasing the total number samples used doesn't effect the estimated Gaussian graph greatly, but it dramatically decreases the height of the data point occurrences of the raw data. This is because the calculation to find the estimated mean remains the same but the number of data that could possibly occur reduces giving a lower total spread across all data values. Having the sample size set to 10,000 means the graphs curve is of a size that demonstrates the point with clarity.

## 5. Generate a default 2D distribution

![default-two-dimensional-distribution-code](../img/default-two-dimensional-distribution-code.png)\

![default-two-dimensional-distribution-graph](../img/default-two-dimensional-distribution-graph.png)\

The task was to graph a representation of a default 2D distribution, which, in essence, is a normal probability distribution but of a higher dimension. At its heart, it derives from the central limit theorem as it shows two independently correlated set of random normal distributed data's mean still relatively equal to the median of the data values.

If you were to reduce the number of samples, it reduces the density of the cluster in the middle and makes the results more sparse. Increasing the standard deviation increases the distance of the data's relation to the mean. Changing the value of the mean shifts the middle of the cluster to the value set.


# TO REMEMBER
- understanding of either rand or randn
- understanding of mean var
- insight into task
- (matlab code)
- What is the task
- How i solve it
- What does it mean

- **Eplain**
- How does changing number samples, bins effect what you see
- Gaussian adding no tegether, doesn't matter what limit they tend to go to a guassain form 
- what constitutes a sensible choice
- model data by estimate parameters

## 1. Generate a noisy line

![noisyLine-code](../img/noisyLine-code.png)\

![noisyLine-graph](../img/noisyLine-graph.png)\


The task was to create and plot a linear regression line with noise. I achieved this by using the equation of a 1D line which stipulates that the line is composed of the gradient of the line, in this case given to us at a value of 1.6, multiplied by the x values with the Y intersect at 0, which was also provided to us at the value of 6, added onto the result of that. The samples to add Gaussian noise to the line was generated using the `randn` function in conjunction with the sample size, stated at 100, the mean, value 0, and standard deviation, value of 1.

This noise generation returns a matrix of normally distributed numbers in a matrix of 1 by the size of samples with a mean and standard deviation of the values stated. The fact we used `randn` gives us the Gaussian noise, instead of using `rand` which would give us uniformly distributed numbers, making the line's noise smoother because the probability of the range of the errors would be evenly distributed. Changing the sample size means we also have to change the range in which the x values are generated in order to make the matrices dimensions match, but in doing so, increasing both makes for a far noisier line, with the gaps between each error point much tighter due to the fact of the increase in data as a whole.

This line demonstrates highly positive correlated data that has noise in the form of errors.

## 2. Implement linear regression from first principles

![FP-LR-code-1](../img/FP-LR-code-1.png)\
![FP-LR-code-2](../img/FP-LR-code-2.png)\
![FP-LR-code-3](../img/FP-LR-code-3.png)\
![FP-LR-code-4](../img/FP-LR-code-4.png)\

![linear-regression-first-principle-graph](../img/linear-regression-first-principle-graph.png)\


The task was to create linear regression line from first principles, this being that we had to program the fundamental methodologies of linear regressions myself, and not to use the in-built Matlab functions. I achieved this by using the least fitting square equation but programmed out step by step. The LFS method can be used to find the 'line of best fit' for a set of correlated data. It does so by minimising the residuals of the points from the curve. I first had to re-create the noisy line as specified in the first question of this practical, again, using `randn` to get the noise for the line as it generates a matrix of normally distributed (Gaussian) values, that being the probability of occurrence tend towards the mean and median of the data. From that I could work out the mean of both axes, mean being the average value over a given data set, which works out to be 1 and ~7 for x and y axes accordingly. The mean was needed to find the deviation of each point of the noisy line away from each axis's mean, deviation being the literal distance from one point to another (distance on each axes current data point from the respective axis mean). These values gave me enough information to calculate the linear regression line with only a few more manipulations to them. First by squaring the x-axis deviations, then multiplying the deviations of each axis together, summing both of these values and dividing together to get the gradient of the line (which works out to be 1.6 as given in the question above) and finally deriving the y-intersect by subtracting the product of the gradient and x-axis mean from the y-axis mean. With the gradient and intersect calculated, you can plug that into the standard y = mx + C equation to get the line. The resulting line essentially reverses the noise giving you a nice smooth line of best fit.

## Fit the test line using your linear regression function

![fitted-noisy-LR-code](../img/fitted-noisy-line-code.png)\

![fitted-noisy-LR-graph](../img/fitted-noisy-LR-graph.png)\

This exercise simply is an amalgamation of the previous two exercises. That is, to plot the 'fitted' line from exercise two (created from first principles using the LFS method) and the noisy linear line from exercise one. This produces a fitted noisy line, emphasising two things. Firstly, that the least fitting square method really does produce a reasonable line of best fit for the data and secondly, that draws attention to the fact the data is so positively correlated.
